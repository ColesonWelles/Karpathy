{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea93251-3da5-43b7-96e9-71ab78bbf6b6",
   "metadata": {},
   "source": [
    "## My review and implementation of a BPE (Byte Pair Encoding) tokenizer as outlined in Andrej Karpathy's Zero To Hero lecture series. \n",
    "\n",
    "In Python, strings are immutable sequences of Unicode code points. What are Unicode code points? Currently it's a definitition of 149,813 characters across 161 scripts as determined by the Unicode Consortium. It defines what they \"look like\" and which integers represent them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce1a365d-baf9-4e93-95a2-a5c865d403d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The ord() function returns the number representing the unicode code of a specified character. \n",
    "ord(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed81c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128293"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"ðŸ”¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcd08e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20013"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"ä¸­\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0d96b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24456, 22810, 20154, 37117, 20250, 35828, 20013, 25991]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x) for x in \"å¾ˆå¤šäººéƒ½ä¼šè¯´ä¸­æ–‡\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c9c406",
   "metadata": {},
   "source": [
    "Naturally, one might think to themselves, why do we need to tokenize a sequence at all when we already have these integers at our disposal? One reason is that our tokenizer vocabulary would be very, very long. The second reason is that the Unicode Standard is \"alive\" and is updated with regularity, which is concerning from a stability point of view. \n",
    "\n",
    "To overcome these issues, we can initially turn to encodings. The Unicode Consortium outlines three options; UTF-8, UTF-16, and UTF-32. UTF-8 is by far the most commonly used. UTF-8 takes every single unicode code point and translates it into a byte stream. This byte stream is between 1 - 4 bytes, thus it's considered a variable length encoding. There are pros/cons and trade-offs with all three encodings, but one the many pro's of UTF-8 is that it is backwards compatible with simple ASCII text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3ca23d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xe5\\xbe\\x88\\xe5\\xa4\\x9a\\xe4\\xba\\xba\\xe9\\x83\\xbd\\xe4\\xbc\\x9a\\xe8\\xaf\\xb4\\xe4\\xb8\\xad\\xe6\\x96\\x87'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"å¾ˆå¤šäººéƒ½ä¼šè¯´ä¸­æ–‡\".encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f5e40b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[229,\n",
       " 190,\n",
       " 136,\n",
       " 229,\n",
       " 164,\n",
       " 154,\n",
       " 228,\n",
       " 186,\n",
       " 186,\n",
       " 233,\n",
       " 131,\n",
       " 189,\n",
       " 228,\n",
       " 188,\n",
       " 154,\n",
       " 232,\n",
       " 175,\n",
       " 180,\n",
       " 228,\n",
       " 184,\n",
       " 173,\n",
       " 230,\n",
       " 150,\n",
       " 135]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"å¾ˆå¤šäººéƒ½ä¼šè¯´ä¸­æ–‡\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99e6e833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[255,\n",
       " 254,\n",
       " 0,\n",
       " 0,\n",
       " 136,\n",
       " 95,\n",
       " 0,\n",
       " 0,\n",
       " 26,\n",
       " 89,\n",
       " 0,\n",
       " 0,\n",
       " 186,\n",
       " 78,\n",
       " 0,\n",
       " 0,\n",
       " 253,\n",
       " 144,\n",
       " 0,\n",
       " 0,\n",
       " 26,\n",
       " 79,\n",
       " 0,\n",
       " 0,\n",
       " 244,\n",
       " 139,\n",
       " 0,\n",
       " 0,\n",
       " 45,\n",
       " 78,\n",
       " 0,\n",
       " 0,\n",
       " 135,\n",
       " 101,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"å¾ˆå¤šäººéƒ½ä¼šè¯´ä¸­æ–‡\".encode(\"utf-32\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0160df",
   "metadata": {},
   "source": [
    "It's somewhat clear that, for our purposes, UTF-32 is rather wasteful (notice all the superflous zeroes). \n",
    "\n",
    "However, if we just naively use UTF-8 byte streams, that would mean we're implicitly working with a vocabulary size of 256, which is far too small. There has been some research published around creating tokenizer-free autoregressive sequence modeling for LLM's, which theoretically would be fantastic. However as it currently stands, our current Transformer attention mechanisms are limited for computational reasons such that feeding UTF-8 encodings is very ineffecient and becomes exceedingly so with longer and longer sequence lengths. \n",
    "\n",
    "## Enter the Byte Pair Encoding algorithm\n",
    "\n",
    "The alogrithm itself is thankfully not very complicated and yet it is incredibly enabling. Let's say we have some sort of an input sequence. We iteratively find the pairs of tokens in that sequnce that occur the most frequently. Once those pairs have been identified, we then replace that pair with a single new token that we append to our vocabularly. \n",
    "\n",
    "### 'aaabdaaabac'(vocab_size=4) -> 'ZabdZabac'(Z=aa) -> 'ZYdZYac'(Y=ab, Z=aa) -> 'XdXac'(X=ZY, Y=ab, Z=aa) \n",
    "Thus we went from a sequence of 11 with a vocab_size 4, and compressed down to a sequence of 5 with a vocab_size of 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e88172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Our goal is to promote usage and support of the UTF-8 encoding and to convince that it should be the default choice of encoding for storing text strings in memory or on disk, for communication and all other uses. We believe that our approach improves performance, reduces complexity of software and helps prevent many Unicode-related bugs. We suggest that other encodings of Unicode (or text, in general) belong to rare edge-cases of optimization and should be avoided by mainstream users.\n",
      "length 489\n",
      "---\n",
      "[79, 117, 114, 32, 103, 111, 97, 108, 32, 105, 115, 32, 116, 111, 32, 112, 114, 111, 109, 111, 116, 101, 32, 117, 115, 97, 103, 101, 32, 97, 110, 100, 32, 115, 117, 112, 112, 111, 114, 116, 32, 111, 102, 32, 116, 104, 101, 32, 85, 84, 70, 45, 56, 32, 101, 110, 99, 111, 100, 105, 110, 103, 32, 97, 110, 100, 32, 116, 111, 32, 99, 111, 110, 118, 105, 110, 99, 101, 32, 116, 104, 97, 116, 32, 105, 116, 32, 115, 104, 111, 117, 108, 100, 32, 98, 101, 32, 116, 104, 101, 32, 100, 101, 102, 97, 117, 108, 116, 32, 99, 104, 111, 105, 99, 101, 32, 111, 102, 32, 101, 110, 99, 111, 100, 105, 110, 103, 32, 102, 111, 114, 32, 115, 116, 111, 114, 105, 110, 103, 32, 116, 101, 120, 116, 32, 115, 116, 114, 105, 110, 103, 115, 32, 105, 110, 32, 109, 101, 109, 111, 114, 121, 32, 111, 114, 32, 111, 110, 32, 100, 105, 115, 107, 44, 32, 102, 111, 114, 32, 99, 111, 109, 109, 117, 110, 105, 99, 97, 116, 105, 111, 110, 32, 97, 110, 100, 32, 97, 108, 108, 32, 111, 116, 104, 101, 114, 32, 117, 115, 101, 115, 46, 32, 87, 101, 32, 98, 101, 108, 105, 101, 118, 101, 32, 116, 104, 97, 116, 32, 111, 117, 114, 32, 97, 112, 112, 114, 111, 97, 99, 104, 32, 105, 109, 112, 114, 111, 118, 101, 115, 32, 112, 101, 114, 102, 111, 114, 109, 97, 110, 99, 101, 44, 32, 114, 101, 100, 117, 99, 101, 115, 32, 99, 111, 109, 112, 108, 101, 120, 105, 116, 121, 32, 111, 102, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 97, 110, 100, 32, 104, 101, 108, 112, 115, 32, 112, 114, 101, 118, 101, 110, 116, 32, 109, 97, 110, 121, 32, 85, 110, 105, 99, 111, 100, 101, 45, 114, 101, 108, 97, 116, 101, 100, 32, 98, 117, 103, 115, 46, 32, 87, 101, 32, 115, 117, 103, 103, 101, 115, 116, 32, 116, 104, 97, 116, 32, 111, 116, 104, 101, 114, 32, 101, 110, 99, 111, 100, 105, 110, 103, 115, 32, 111, 102, 32, 85, 110, 105, 99, 111, 100, 101, 32, 40, 111, 114, 32, 116, 101, 120, 116, 44, 32, 105, 110, 32, 103, 101, 110, 101, 114, 97, 108, 41, 32, 98, 101, 108, 111, 110, 103, 32, 116, 111, 32, 114, 97, 114, 101, 32, 101, 100, 103, 101, 45, 99, 97, 115, 101, 115, 32, 111, 102, 32, 111, 112, 116, 105, 109, 105, 122, 97, 116, 105, 111, 110, 32, 97, 110, 100, 32, 115, 104, 111, 117, 108, 100, 32, 98, 101, 32, 97, 118, 111, 105, 100, 101, 100, 32, 98, 121, 32, 109, 97, 105, 110, 115, 116, 114, 101, 97, 109, 32, 117, 115, 101, 114, 115, 46]\n",
      "length 489\n"
     ]
    }
   ],
   "source": [
    "# text from the first full paragraph of http://utf8everywhere.org/\n",
    "text = \"Our goal is to promote usage and support of the UTF-8 encoding and to convince that it should be the default choice of encoding for storing text strings in memory or on disk, for communication and all other uses. We believe that our approach improves performance, reduces complexity of software and helps prevent many Unicode-related bugs. We suggest that other encodings of Unicode (or text, in general) belong to rare edge-cases of optimization and should be avoided by mainstream users.\"\n",
    "tokens = text.encode(\"utf-8\") # our raw bytes\n",
    "tokens = list(map(int, tokens)) # converts our raw bytes to a list of int's in rang 0 - 255 for this example\n",
    "print('---')\n",
    "print(text)\n",
    "print(\"length\", len(text))\n",
    "print('---')\n",
    "print(tokens)\n",
    "print(\"length\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab2bf679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(79, 117): 1, (117, 114): 2, (114, 32): 8, (32, 103): 2, (103, 111): 1, (111, 97): 2, (97, 108): 3, (108, 32): 2, (32, 105): 5, (105, 115): 2, (115, 32): 7, (32, 116): 10, (116, 111): 4, (111, 32): 3, (32, 112): 3, (112, 114): 4, (114, 111): 3, (111, 109): 3, (109, 111): 2, (111, 116): 3, (116, 101): 4, (101, 32): 14, (32, 117): 3, (117, 115): 3, (115, 97): 1, (97, 103): 1, (103, 101): 4, (32, 97): 8, (97, 110): 7, (110, 100): 5, (100, 32): 9, (32, 115): 7, (115, 117): 2, (117, 112): 1, (112, 112): 2, (112, 111): 1, (111, 114): 8, (114, 116): 1, (116, 32): 9, (32, 111): 11, (111, 102): 6, (102, 32): 5, (116, 104): 7, (104, 101): 5, (32, 85): 3, (85, 84): 1, (84, 70): 1, (70, 45): 1, (45, 56): 1, (56, 32): 1, (32, 101): 4, (101, 110): 5, (110, 99): 5, (99, 111): 8, (111, 100): 5, (100, 105): 4, (105, 110): 9, (110, 103): 6, (103, 32): 4, (32, 99): 4, (111, 110): 5, (110, 118): 1, (118, 105): 1, (99, 101): 4, (104, 97): 3, (97, 116): 6, (105, 116): 2, (115, 104): 2, (104, 111): 3, (111, 117): 3, (117, 108): 3, (108, 100): 2, (32, 98): 6, (98, 101): 4, (32, 100): 2, (100, 101): 4, (101, 102): 1, (102, 97): 1, (97, 117): 1, (108, 116): 1, (99, 104): 2, (111, 105): 2, (105, 99): 4, (32, 102): 2, (102, 111): 3, (115, 116): 4, (114, 105): 2, (101, 120): 3, (120, 116): 2, (116, 114): 2, (103, 115): 3, (110, 32): 5, (32, 109): 3, (109, 101): 1, (101, 109): 1, (114, 121): 1, (121, 32): 4, (115, 107): 1, (107, 44): 1, (44, 32): 3, (109, 109): 1, (109, 117): 1, (117, 110): 1, (110, 105): 3, (99, 97): 2, (116, 105): 3, (105, 111): 2, (108, 108): 1, (101, 114): 5, (115, 101): 3, (101, 115): 5, (115, 46): 3, (46, 32): 2, (32, 87): 2, (87, 101): 2, (101, 108): 4, (108, 105): 1, (105, 101): 1, (101, 118): 2, (118, 101): 3, (97, 112): 1, (97, 99): 1, (104, 32): 1, (105, 109): 2, (109, 112): 2, (111, 118): 1, (112, 101): 1, (114, 102): 1, (114, 109): 1, (109, 97): 3, (101, 44): 1, (32, 114): 2, (114, 101): 6, (101, 100): 4, (100, 117): 1, (117, 99): 1, (112, 108): 1, (108, 101): 1, (120, 105): 1, (116, 121): 1, (115, 111): 1, (102, 116): 1, (116, 119): 1, (119, 97): 1, (97, 114): 2, (32, 104): 1, (108, 112): 1, (112, 115): 1, (110, 116): 1, (110, 121): 1, (85, 110): 2, (101, 45): 2, (45, 114): 1, (108, 97): 1, (98, 117): 1, (117, 103): 2, (103, 103): 1, (32, 40): 1, (40, 111): 1, (116, 44): 1, (110, 101): 1, (114, 97): 2, (108, 41): 1, (41, 32): 1, (108, 111): 1, (100, 103): 1, (45, 99): 1, (97, 115): 1, (111, 112): 1, (112, 116): 1, (109, 105): 1, (105, 122): 1, (122, 97): 1, (97, 118): 1, (118, 111): 1, (105, 100): 1, (98, 121): 1, (97, 105): 1, (110, 115): 1, (101, 97): 1, (97, 109): 1, (109, 32): 1, (114, 115): 1}\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # A pythonic way to iterate consecutive elements in parallel\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "print(stats)\n",
    "# print(sorted(((v,k) for k,v in stats.items()), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52e5461b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14, (101, 32)), (11, (32, 111)), (10, (32, 116)), (9, (116, 32)), (9, (105, 110)), (9, (100, 32)), (8, (114, 32)), (8, (111, 114)), (8, (99, 111)), (8, (32, 97)), (7, (116, 104)), (7, (115, 32)), (7, (97, 110)), (7, (32, 115)), (6, (114, 101)), (6, (111, 102)), (6, (110, 103)), (6, (97, 116)), (6, (32, 98)), (5, (111, 110)), (5, (111, 100)), (5, (110, 100)), (5, (110, 99)), (5, (110, 32)), (5, (104, 101)), (5, (102, 32)), (5, (101, 115)), (5, (101, 114)), (5, (101, 110)), (5, (32, 105)), (4, (121, 32)), (4, (116, 111)), (4, (116, 101)), (4, (115, 116)), (4, (112, 114)), (4, (105, 99)), (4, (103, 101)), (4, (103, 32)), (4, (101, 108)), (4, (101, 100)), (4, (100, 105)), (4, (100, 101)), (4, (99, 101)), (4, (98, 101)), (4, (32, 101)), (4, (32, 99)), (3, (118, 101)), (3, (117, 115)), (3, (117, 108)), (3, (116, 105)), (3, (115, 101)), (3, (115, 46)), (3, (114, 111)), (3, (111, 117)), (3, (111, 116)), (3, (111, 109)), (3, (111, 32)), (3, (110, 105)), (3, (109, 97)), (3, (104, 111)), (3, (104, 97)), (3, (103, 115)), (3, (102, 111)), (3, (101, 120)), (3, (97, 108)), (3, (44, 32)), (3, (32, 117)), (3, (32, 112)), (3, (32, 109)), (3, (32, 85)), (2, (120, 116)), (2, (117, 114)), (2, (117, 103)), (2, (116, 114)), (2, (115, 117)), (2, (115, 104)), (2, (114, 105)), (2, (114, 97)), (2, (112, 112)), (2, (111, 105)), (2, (111, 97)), (2, (109, 112)), (2, (109, 111)), (2, (108, 100)), (2, (108, 32)), (2, (105, 116)), (2, (105, 115)), (2, (105, 111)), (2, (105, 109)), (2, (101, 118)), (2, (101, 45)), (2, (99, 104)), (2, (99, 97)), (2, (97, 114)), (2, (87, 101)), (2, (85, 110)), (2, (46, 32)), (2, (32, 114)), (2, (32, 103)), (2, (32, 102)), (2, (32, 100)), (2, (32, 87)), (1, (122, 97)), (1, (120, 105)), (1, (119, 97)), (1, (118, 111)), (1, (118, 105)), (1, (117, 112)), (1, (117, 110)), (1, (117, 99)), (1, (116, 121)), (1, (116, 119)), (1, (116, 44)), (1, (115, 111)), (1, (115, 107)), (1, (115, 97)), (1, (114, 121)), (1, (114, 116)), (1, (114, 115)), (1, (114, 109)), (1, (114, 102)), (1, (112, 116)), (1, (112, 115)), (1, (112, 111)), (1, (112, 108)), (1, (112, 101)), (1, (111, 118)), (1, (111, 112)), (1, (110, 121)), (1, (110, 118)), (1, (110, 116)), (1, (110, 115)), (1, (110, 101)), (1, (109, 117)), (1, (109, 109)), (1, (109, 105)), (1, (109, 101)), (1, (109, 32)), (1, (108, 116)), (1, (108, 112)), (1, (108, 111)), (1, (108, 108)), (1, (108, 105)), (1, (108, 101)), (1, (108, 97)), (1, (108, 41)), (1, (107, 44)), (1, (105, 122)), (1, (105, 101)), (1, (105, 100)), (1, (104, 32)), (1, (103, 111)), (1, (103, 103)), (1, (102, 116)), (1, (102, 97)), (1, (101, 109)), (1, (101, 102)), (1, (101, 97)), (1, (101, 44)), (1, (100, 117)), (1, (100, 103)), (1, (98, 121)), (1, (98, 117)), (1, (97, 118)), (1, (97, 117)), (1, (97, 115)), (1, (97, 112)), (1, (97, 109)), (1, (97, 105)), (1, (97, 103)), (1, (97, 99)), (1, (85, 84)), (1, (84, 70)), (1, (79, 117)), (1, (70, 45)), (1, (56, 32)), (1, (45, 114)), (1, (45, 99)), (1, (45, 56)), (1, (41, 32)), (1, (40, 111)), (1, (32, 104)), (1, (32, 40))]\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # A pythonic way to iterate consecutive elements in parallel\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "# print(stats)\n",
    "print(sorted(((v,k) for k,v in stats.items()), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11649f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', ' ')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chr() can kinda be thought of the opposite of ord()\n",
    "chr(101), chr(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13d6999",
   "metadata": {},
   "source": [
    "Unsurprisingly to those familiar with English, there is a lot of sequences of \"e\" followed by \" \" (space) in a given paragraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "766e31dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # A pythonic way to iterate consecutive elements in parallel\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "# print(stats)\n",
    "# print(sorted(((v,k) for k,v in stats.items()), reverse=True))\n",
    "\n",
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aafd89b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 99, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if we are not at the very last position AND the pair matches, replace it\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2 \n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))\n",
    "\n",
    "#tokens2 = merge(tokens, top_pair, 256)\n",
    "#print(tokens2)\n",
    "#print(\"length:\", len(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e6eaf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79, 117, 114, 32, 103, 111, 97, 108, 32, 105, 115, 32, 116, 111, 32, 112, 114, 111, 109, 111, 116, 256, 117, 115, 97, 103, 256, 97, 110, 100, 32, 115, 117, 112, 112, 111, 114, 116, 32, 111, 102, 32, 116, 104, 256, 85, 84, 70, 45, 56, 32, 101, 110, 99, 111, 100, 105, 110, 103, 32, 97, 110, 100, 32, 116, 111, 32, 99, 111, 110, 118, 105, 110, 99, 256, 116, 104, 97, 116, 32, 105, 116, 32, 115, 104, 111, 117, 108, 100, 32, 98, 256, 116, 104, 256, 100, 101, 102, 97, 117, 108, 116, 32, 99, 104, 111, 105, 99, 256, 111, 102, 32, 101, 110, 99, 111, 100, 105, 110, 103, 32, 102, 111, 114, 32, 115, 116, 111, 114, 105, 110, 103, 32, 116, 101, 120, 116, 32, 115, 116, 114, 105, 110, 103, 115, 32, 105, 110, 32, 109, 101, 109, 111, 114, 121, 32, 111, 114, 32, 111, 110, 32, 100, 105, 115, 107, 44, 32, 102, 111, 114, 32, 99, 111, 109, 109, 117, 110, 105, 99, 97, 116, 105, 111, 110, 32, 97, 110, 100, 32, 97, 108, 108, 32, 111, 116, 104, 101, 114, 32, 117, 115, 101, 115, 46, 32, 87, 256, 98, 101, 108, 105, 101, 118, 256, 116, 104, 97, 116, 32, 111, 117, 114, 32, 97, 112, 112, 114, 111, 97, 99, 104, 32, 105, 109, 112, 114, 111, 118, 101, 115, 32, 112, 101, 114, 102, 111, 114, 109, 97, 110, 99, 101, 44, 32, 114, 101, 100, 117, 99, 101, 115, 32, 99, 111, 109, 112, 108, 101, 120, 105, 116, 121, 32, 111, 102, 32, 115, 111, 102, 116, 119, 97, 114, 256, 97, 110, 100, 32, 104, 101, 108, 112, 115, 32, 112, 114, 101, 118, 101, 110, 116, 32, 109, 97, 110, 121, 32, 85, 110, 105, 99, 111, 100, 101, 45, 114, 101, 108, 97, 116, 101, 100, 32, 98, 117, 103, 115, 46, 32, 87, 256, 115, 117, 103, 103, 101, 115, 116, 32, 116, 104, 97, 116, 32, 111, 116, 104, 101, 114, 32, 101, 110, 99, 111, 100, 105, 110, 103, 115, 32, 111, 102, 32, 85, 110, 105, 99, 111, 100, 256, 40, 111, 114, 32, 116, 101, 120, 116, 44, 32, 105, 110, 32, 103, 101, 110, 101, 114, 97, 108, 41, 32, 98, 101, 108, 111, 110, 103, 32, 116, 111, 32, 114, 97, 114, 256, 101, 100, 103, 101, 45, 99, 97, 115, 101, 115, 32, 111, 102, 32, 111, 112, 116, 105, 109, 105, 122, 97, 116, 105, 111, 110, 32, 97, 110, 100, 32, 115, 104, 111, 117, 108, 100, 32, 98, 256, 97, 118, 111, 105, 100, 101, 100, 32, 98, 121, 32, 109, 97, 105, 110, 115, 116, 114, 101, 97, 109, 32, 117, 115, 101, 114, 115, 46]\n",
      "length: 475\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if we are not at the very last position AND the pair matches, replace it\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2 \n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "#print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))\n",
    "\n",
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "print(tokens2)\n",
    "print(\"length:\", len(tokens2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139aba82",
   "metadata": {},
   "source": [
    "We did a single pass through where we replaced 14 pairs (the (101,32) pair) and thus reduced our tokens from 489 - 14 to get 475."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3f65612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into new token 256\n",
      "merging (32, 111) into new token 257\n",
      "merging (100, 32) into new token 258\n",
      "merging (105, 110) into new token 259\n",
      "merging (99, 111) into new token 260\n",
      "merging (114, 32) into new token 261\n",
      "merging (97, 110) into new token 262\n",
      "merging (116, 104) into new token 263\n",
      "merging (97, 116) into new token 264\n",
      "merging (115, 32) into new token 265\n",
      "merging (262, 258) into new token 266\n",
      "merging (101, 110) into new token 267\n",
      "merging (260, 100) into new token 268\n",
      "merging (259, 103) into new token 269\n",
      "merging (116, 32) into new token 270\n",
      "merging (116, 111) into new token 271\n",
      "merging (112, 114) into new token 272\n",
      "merging (257, 102) into new token 273\n",
      "merging (258, 98) into new token 274\n",
      "merging (101, 108) into new token 275\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # A pythonic way to iterate consecutive elements in parallel\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if we are not at the very last position AND the pair matches, replace it\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2 \n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "# ---\n",
    "vocab_size = 276 # our desired final vocab size for this example\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens) # makes a copy so we don't destroy the original list\n",
    "\n",
    "merges = {} # (int, int) -> int | child1, child2 merges\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    print(f\"merging {pair} into new token {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc112280",
   "metadata": {},
   "source": [
    "Notice that even our newly minted tokens are, in turn, eligible for merging as we continue to interate through. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09ca9899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 489\n",
      "ids length: 362\n",
      "compression ratio: 1.35X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"ids length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3237ed2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
