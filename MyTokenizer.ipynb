{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea93251-3da5-43b7-96e9-71ab78bbf6b6",
   "metadata": {},
   "source": [
    "## My review and implementation of a BPE (Byte Pair Encoding) tokenizer as outlined in Andrej Karpathy's Zero To Hero lecture series. \n",
    "\n",
    "In Python, strings are immutable sequences of Unicode code points. What are Unicode code points? Currently it's a definitition of 149,813 characters across 161 scripts as determined by the Unicode Consortium. It defines what they \"look like\" and which integers represent them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce1a365d-baf9-4e93-95a2-a5c865d403d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The ord() function returns the number representing the unicode code of a specified character. \n",
    "ord(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed81c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128293"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"ðŸ”¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcd08e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20013"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"ä¸­\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0d96b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24456, 22810, 20154, 37117, 20250, 35828, 20013, 25991]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x) for x in \"å¾ˆå¤šäººéƒ½ä¼šè¯´ä¸­æ–‡\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c9c406",
   "metadata": {},
   "source": [
    "Naturally, one might think to themselves, why do we need to tokenize a sequence at all when we already have these integers at our disposal? One reason is that our tokenizer vocabulary would be very, very long. The second reason is that the Unicode Standard is \"alive\" and is updated with regularity, which is concerning from a stability point of view. \n",
    "\n",
    "To overcome these issues, we can initially turn to encodings. The Unicode Consortium outlines three options; UTF-8, UTF-16, and UTF-32. UTF-8 is by far the most commonly used. UTF-8 takes every single unicode code point and translates it into a byte stream. This byte stream is between 1 - 4 bytes, thus it's considered a variable length encoding. There are pros/cons and trade-offs with all three encodings, but one the many pro's of UTF-8 is that it is backwards compatible with simple ASCII text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3ca23d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xe5\\xbe\\x88\\xe5\\xa4\\x9a\\xe4\\xba\\xba\\xe9\\x83\\xbd\\xe4\\xbc\\x9a\\xe8\\xaf\\xb4\\xe4\\xb8\\xad\\xe6\\x96\\x87'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"å¾ˆå¤šäººéƒ½ä¼šè¯´ä¸­æ–‡\".encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f5e40b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[229,\n",
       " 190,\n",
       " 136,\n",
       " 229,\n",
       " 164,\n",
       " 154,\n",
       " 228,\n",
       " 186,\n",
       " 186,\n",
       " 233,\n",
       " 131,\n",
       " 189,\n",
       " 228,\n",
       " 188,\n",
       " 154,\n",
       " 232,\n",
       " 175,\n",
       " 180,\n",
       " 228,\n",
       " 184,\n",
       " 173,\n",
       " 230,\n",
       " 150,\n",
       " 135]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"å¾ˆå¤šäººéƒ½ä¼šè¯´ä¸­æ–‡\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99e6e833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[255,\n",
       " 254,\n",
       " 0,\n",
       " 0,\n",
       " 136,\n",
       " 95,\n",
       " 0,\n",
       " 0,\n",
       " 26,\n",
       " 89,\n",
       " 0,\n",
       " 0,\n",
       " 186,\n",
       " 78,\n",
       " 0,\n",
       " 0,\n",
       " 253,\n",
       " 144,\n",
       " 0,\n",
       " 0,\n",
       " 26,\n",
       " 79,\n",
       " 0,\n",
       " 0,\n",
       " 244,\n",
       " 139,\n",
       " 0,\n",
       " 0,\n",
       " 45,\n",
       " 78,\n",
       " 0,\n",
       " 0,\n",
       " 135,\n",
       " 101,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"å¾ˆå¤šäººéƒ½ä¼šè¯´ä¸­æ–‡\".encode(\"utf-32\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0160df",
   "metadata": {},
   "source": [
    "It's somewhat clear that, for our purposes, UTF-32 is rather wasteful (notice all the superflous zeroes). \n",
    "\n",
    "However, if we just naively use UTF-8 byte streams, that would mean we're implicitly working with a vocabulary size of 256, which is far too small. There has been some research published around creating tokenizer-free autoregressive sequence modeling for LLM's, which theoretically would be fantastic. However as it currently stands, our current Transformer attention mechanisms are limited for computational reasons such that feeding UTF-8 encodings is very ineffecient and becomes exceedingly so with longer and longer sequence lengths. \n",
    "\n",
    "## Enter the Byte Pair Encoding algorithm\n",
    "\n",
    "The alogrithm itself is thankfully not very complicated and yet it is incredibly enabling. Let's say we have some sort of an input sequence. We iteratively find the pairs of tokens in that sequnce that occur the most frequently. Once those pairs have been identified, we then replace that pair with a single new token that we append to our vocabularly. \n",
    "\n",
    "### 'aaabdaaabac'(vocab_size=4) -> 'ZabdZabac'(Z=aa) -> 'ZYdZYac'(Y=ab, Z=aa) -> 'XdXac'(X=ZY, Y=ab, Z=aa) \n",
    "Thus we went from a sequence of 11 with a vocab_size 4, and compressed down to a sequence of 5 with a vocab_size of 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e88172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Our goal is to promote usage and support of the UTF-8 encoding and to convince that it should be the default choice of encoding for storing text strings in memory or on disk, for communication and all other uses. We believe that our approach improves performance, reduces complexity of software and helps prevent many Unicode-related bugs. We suggest that other encodings of Unicode (or text, in general) belong to rare edge-cases of optimization and should be avoided by mainstream users.\n",
      "length 489\n",
      "---\n",
      "[79, 117, 114, 32, 103, 111, 97, 108, 32, 105, 115, 32, 116, 111, 32, 112, 114, 111, 109, 111, 116, 101, 32, 117, 115, 97, 103, 101, 32, 97, 110, 100, 32, 115, 117, 112, 112, 111, 114, 116, 32, 111, 102, 32, 116, 104, 101, 32, 85, 84, 70, 45, 56, 32, 101, 110, 99, 111, 100, 105, 110, 103, 32, 97, 110, 100, 32, 116, 111, 32, 99, 111, 110, 118, 105, 110, 99, 101, 32, 116, 104, 97, 116, 32, 105, 116, 32, 115, 104, 111, 117, 108, 100, 32, 98, 101, 32, 116, 104, 101, 32, 100, 101, 102, 97, 117, 108, 116, 32, 99, 104, 111, 105, 99, 101, 32, 111, 102, 32, 101, 110, 99, 111, 100, 105, 110, 103, 32, 102, 111, 114, 32, 115, 116, 111, 114, 105, 110, 103, 32, 116, 101, 120, 116, 32, 115, 116, 114, 105, 110, 103, 115, 32, 105, 110, 32, 109, 101, 109, 111, 114, 121, 32, 111, 114, 32, 111, 110, 32, 100, 105, 115, 107, 44, 32, 102, 111, 114, 32, 99, 111, 109, 109, 117, 110, 105, 99, 97, 116, 105, 111, 110, 32, 97, 110, 100, 32, 97, 108, 108, 32, 111, 116, 104, 101, 114, 32, 117, 115, 101, 115, 46, 32, 87, 101, 32, 98, 101, 108, 105, 101, 118, 101, 32, 116, 104, 97, 116, 32, 111, 117, 114, 32, 97, 112, 112, 114, 111, 97, 99, 104, 32, 105, 109, 112, 114, 111, 118, 101, 115, 32, 112, 101, 114, 102, 111, 114, 109, 97, 110, 99, 101, 44, 32, 114, 101, 100, 117, 99, 101, 115, 32, 99, 111, 109, 112, 108, 101, 120, 105, 116, 121, 32, 111, 102, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 97, 110, 100, 32, 104, 101, 108, 112, 115, 32, 112, 114, 101, 118, 101, 110, 116, 32, 109, 97, 110, 121, 32, 85, 110, 105, 99, 111, 100, 101, 45, 114, 101, 108, 97, 116, 101, 100, 32, 98, 117, 103, 115, 46, 32, 87, 101, 32, 115, 117, 103, 103, 101, 115, 116, 32, 116, 104, 97, 116, 32, 111, 116, 104, 101, 114, 32, 101, 110, 99, 111, 100, 105, 110, 103, 115, 32, 111, 102, 32, 85, 110, 105, 99, 111, 100, 101, 32, 40, 111, 114, 32, 116, 101, 120, 116, 44, 32, 105, 110, 32, 103, 101, 110, 101, 114, 97, 108, 41, 32, 98, 101, 108, 111, 110, 103, 32, 116, 111, 32, 114, 97, 114, 101, 32, 101, 100, 103, 101, 45, 99, 97, 115, 101, 115, 32, 111, 102, 32, 111, 112, 116, 105, 109, 105, 122, 97, 116, 105, 111, 110, 32, 97, 110, 100, 32, 115, 104, 111, 117, 108, 100, 32, 98, 101, 32, 97, 118, 111, 105, 100, 101, 100, 32, 98, 121, 32, 109, 97, 105, 110, 115, 116, 114, 101, 97, 109, 32, 117, 115, 101, 114, 115, 46]\n",
      "length 489\n"
     ]
    }
   ],
   "source": [
    "# text from the first full paragraph of http://utf8everywhere.org/\n",
    "text = \"Our goal is to promote usage and support of the UTF-8 encoding and to convince that it should be the default choice of encoding for storing text strings in memory or on disk, for communication and all other uses. We believe that our approach improves performance, reduces complexity of software and helps prevent many Unicode-related bugs. We suggest that other encodings of Unicode (or text, in general) belong to rare edge-cases of optimization and should be avoided by mainstream users.\"\n",
    "tokens = text.encode(\"utf-8\") # our raw bytes\n",
    "tokens = list(map(int, tokens)) # converts our raw bytes to a list of int's in rang 0 - 255 for this example\n",
    "print('---')\n",
    "print(text)\n",
    "print(\"length\", len(text))\n",
    "print('---')\n",
    "print(tokens)\n",
    "print(\"length\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab2bf679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(79, 117): 1, (117, 114): 2, (114, 32): 8, (32, 103): 2, (103, 111): 1, (111, 97): 2, (97, 108): 3, (108, 32): 2, (32, 105): 5, (105, 115): 2, (115, 32): 7, (32, 116): 10, (116, 111): 4, (111, 32): 3, (32, 112): 3, (112, 114): 4, (114, 111): 3, (111, 109): 3, (109, 111): 2, (111, 116): 3, (116, 101): 4, (101, 32): 14, (32, 117): 3, (117, 115): 3, (115, 97): 1, (97, 103): 1, (103, 101): 4, (32, 97): 8, (97, 110): 7, (110, 100): 5, (100, 32): 9, (32, 115): 7, (115, 117): 2, (117, 112): 1, (112, 112): 2, (112, 111): 1, (111, 114): 8, (114, 116): 1, (116, 32): 9, (32, 111): 11, (111, 102): 6, (102, 32): 5, (116, 104): 7, (104, 101): 5, (32, 85): 3, (85, 84): 1, (84, 70): 1, (70, 45): 1, (45, 56): 1, (56, 32): 1, (32, 101): 4, (101, 110): 5, (110, 99): 5, (99, 111): 8, (111, 100): 5, (100, 105): 4, (105, 110): 9, (110, 103): 6, (103, 32): 4, (32, 99): 4, (111, 110): 5, (110, 118): 1, (118, 105): 1, (99, 101): 4, (104, 97): 3, (97, 116): 6, (105, 116): 2, (115, 104): 2, (104, 111): 3, (111, 117): 3, (117, 108): 3, (108, 100): 2, (32, 98): 6, (98, 101): 4, (32, 100): 2, (100, 101): 4, (101, 102): 1, (102, 97): 1, (97, 117): 1, (108, 116): 1, (99, 104): 2, (111, 105): 2, (105, 99): 4, (32, 102): 2, (102, 111): 3, (115, 116): 4, (114, 105): 2, (101, 120): 3, (120, 116): 2, (116, 114): 2, (103, 115): 3, (110, 32): 5, (32, 109): 3, (109, 101): 1, (101, 109): 1, (114, 121): 1, (121, 32): 4, (115, 107): 1, (107, 44): 1, (44, 32): 3, (109, 109): 1, (109, 117): 1, (117, 110): 1, (110, 105): 3, (99, 97): 2, (116, 105): 3, (105, 111): 2, (108, 108): 1, (101, 114): 5, (115, 101): 3, (101, 115): 5, (115, 46): 3, (46, 32): 2, (32, 87): 2, (87, 101): 2, (101, 108): 4, (108, 105): 1, (105, 101): 1, (101, 118): 2, (118, 101): 3, (97, 112): 1, (97, 99): 1, (104, 32): 1, (105, 109): 2, (109, 112): 2, (111, 118): 1, (112, 101): 1, (114, 102): 1, (114, 109): 1, (109, 97): 3, (101, 44): 1, (32, 114): 2, (114, 101): 6, (101, 100): 4, (100, 117): 1, (117, 99): 1, (112, 108): 1, (108, 101): 1, (120, 105): 1, (116, 121): 1, (115, 111): 1, (102, 116): 1, (116, 119): 1, (119, 97): 1, (97, 114): 2, (32, 104): 1, (108, 112): 1, (112, 115): 1, (110, 116): 1, (110, 121): 1, (85, 110): 2, (101, 45): 2, (45, 114): 1, (108, 97): 1, (98, 117): 1, (117, 103): 2, (103, 103): 1, (32, 40): 1, (40, 111): 1, (116, 44): 1, (110, 101): 1, (114, 97): 2, (108, 41): 1, (41, 32): 1, (108, 111): 1, (100, 103): 1, (45, 99): 1, (97, 115): 1, (111, 112): 1, (112, 116): 1, (109, 105): 1, (105, 122): 1, (122, 97): 1, (97, 118): 1, (118, 111): 1, (105, 100): 1, (98, 121): 1, (97, 105): 1, (110, 115): 1, (101, 97): 1, (97, 109): 1, (109, 32): 1, (114, 115): 1}\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # A pythonic way to iterate consecutive elements in parallel\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "print(stats)\n",
    "# print(sorted(((v,k) for k,v in stats.items()), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52e5461b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14, (101, 32)), (11, (32, 111)), (10, (32, 116)), (9, (116, 32)), (9, (105, 110)), (9, (100, 32)), (8, (114, 32)), (8, (111, 114)), (8, (99, 111)), (8, (32, 97)), (7, (116, 104)), (7, (115, 32)), (7, (97, 110)), (7, (32, 115)), (6, (114, 101)), (6, (111, 102)), (6, (110, 103)), (6, (97, 116)), (6, (32, 98)), (5, (111, 110)), (5, (111, 100)), (5, (110, 100)), (5, (110, 99)), (5, (110, 32)), (5, (104, 101)), (5, (102, 32)), (5, (101, 115)), (5, (101, 114)), (5, (101, 110)), (5, (32, 105)), (4, (121, 32)), (4, (116, 111)), (4, (116, 101)), (4, (115, 116)), (4, (112, 114)), (4, (105, 99)), (4, (103, 101)), (4, (103, 32)), (4, (101, 108)), (4, (101, 100)), (4, (100, 105)), (4, (100, 101)), (4, (99, 101)), (4, (98, 101)), (4, (32, 101)), (4, (32, 99)), (3, (118, 101)), (3, (117, 115)), (3, (117, 108)), (3, (116, 105)), (3, (115, 101)), (3, (115, 46)), (3, (114, 111)), (3, (111, 117)), (3, (111, 116)), (3, (111, 109)), (3, (111, 32)), (3, (110, 105)), (3, (109, 97)), (3, (104, 111)), (3, (104, 97)), (3, (103, 115)), (3, (102, 111)), (3, (101, 120)), (3, (97, 108)), (3, (44, 32)), (3, (32, 117)), (3, (32, 112)), (3, (32, 109)), (3, (32, 85)), (2, (120, 116)), (2, (117, 114)), (2, (117, 103)), (2, (116, 114)), (2, (115, 117)), (2, (115, 104)), (2, (114, 105)), (2, (114, 97)), (2, (112, 112)), (2, (111, 105)), (2, (111, 97)), (2, (109, 112)), (2, (109, 111)), (2, (108, 100)), (2, (108, 32)), (2, (105, 116)), (2, (105, 115)), (2, (105, 111)), (2, (105, 109)), (2, (101, 118)), (2, (101, 45)), (2, (99, 104)), (2, (99, 97)), (2, (97, 114)), (2, (87, 101)), (2, (85, 110)), (2, (46, 32)), (2, (32, 114)), (2, (32, 103)), (2, (32, 102)), (2, (32, 100)), (2, (32, 87)), (1, (122, 97)), (1, (120, 105)), (1, (119, 97)), (1, (118, 111)), (1, (118, 105)), (1, (117, 112)), (1, (117, 110)), (1, (117, 99)), (1, (116, 121)), (1, (116, 119)), (1, (116, 44)), (1, (115, 111)), (1, (115, 107)), (1, (115, 97)), (1, (114, 121)), (1, (114, 116)), (1, (114, 115)), (1, (114, 109)), (1, (114, 102)), (1, (112, 116)), (1, (112, 115)), (1, (112, 111)), (1, (112, 108)), (1, (112, 101)), (1, (111, 118)), (1, (111, 112)), (1, (110, 121)), (1, (110, 118)), (1, (110, 116)), (1, (110, 115)), (1, (110, 101)), (1, (109, 117)), (1, (109, 109)), (1, (109, 105)), (1, (109, 101)), (1, (109, 32)), (1, (108, 116)), (1, (108, 112)), (1, (108, 111)), (1, (108, 108)), (1, (108, 105)), (1, (108, 101)), (1, (108, 97)), (1, (108, 41)), (1, (107, 44)), (1, (105, 122)), (1, (105, 101)), (1, (105, 100)), (1, (104, 32)), (1, (103, 111)), (1, (103, 103)), (1, (102, 116)), (1, (102, 97)), (1, (101, 109)), (1, (101, 102)), (1, (101, 97)), (1, (101, 44)), (1, (100, 117)), (1, (100, 103)), (1, (98, 121)), (1, (98, 117)), (1, (97, 118)), (1, (97, 117)), (1, (97, 115)), (1, (97, 112)), (1, (97, 109)), (1, (97, 105)), (1, (97, 103)), (1, (97, 99)), (1, (85, 84)), (1, (84, 70)), (1, (79, 117)), (1, (70, 45)), (1, (56, 32)), (1, (45, 114)), (1, (45, 99)), (1, (45, 56)), (1, (41, 32)), (1, (40, 111)), (1, (32, 104)), (1, (32, 40))]\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # A pythonic way to iterate consecutive elements in parallel\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "# print(stats)\n",
    "print(sorted(((v,k) for k,v in stats.items()), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11649f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', ' ')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chr() can kinda be thought of the opposite of ord()\n",
    "chr(101), chr(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13d6999",
   "metadata": {},
   "source": [
    "Unsurprisingly to those familiar with English, there is a lot of sequences of \"e\" followed by \" \" (space) in a given paragraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "766e31dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # A pythonic way to iterate consecutive elements in parallel\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "# print(stats)\n",
    "# print(sorted(((v,k) for k,v in stats.items()), reverse=True))\n",
    "\n",
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aafd89b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 99, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if we are not at the very last position AND the pair matches, replace it\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2 \n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))\n",
    "\n",
    "#tokens2 = merge(tokens, top_pair, 256)\n",
    "#print(tokens2)\n",
    "#print(\"length:\", len(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e6eaf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79, 117, 114, 32, 103, 111, 97, 108, 32, 105, 115, 32, 116, 111, 32, 112, 114, 111, 109, 111, 116, 256, 117, 115, 97, 103, 256, 97, 110, 100, 32, 115, 117, 112, 112, 111, 114, 116, 32, 111, 102, 32, 116, 104, 256, 85, 84, 70, 45, 56, 32, 101, 110, 99, 111, 100, 105, 110, 103, 32, 97, 110, 100, 32, 116, 111, 32, 99, 111, 110, 118, 105, 110, 99, 256, 116, 104, 97, 116, 32, 105, 116, 32, 115, 104, 111, 117, 108, 100, 32, 98, 256, 116, 104, 256, 100, 101, 102, 97, 117, 108, 116, 32, 99, 104, 111, 105, 99, 256, 111, 102, 32, 101, 110, 99, 111, 100, 105, 110, 103, 32, 102, 111, 114, 32, 115, 116, 111, 114, 105, 110, 103, 32, 116, 101, 120, 116, 32, 115, 116, 114, 105, 110, 103, 115, 32, 105, 110, 32, 109, 101, 109, 111, 114, 121, 32, 111, 114, 32, 111, 110, 32, 100, 105, 115, 107, 44, 32, 102, 111, 114, 32, 99, 111, 109, 109, 117, 110, 105, 99, 97, 116, 105, 111, 110, 32, 97, 110, 100, 32, 97, 108, 108, 32, 111, 116, 104, 101, 114, 32, 117, 115, 101, 115, 46, 32, 87, 256, 98, 101, 108, 105, 101, 118, 256, 116, 104, 97, 116, 32, 111, 117, 114, 32, 97, 112, 112, 114, 111, 97, 99, 104, 32, 105, 109, 112, 114, 111, 118, 101, 115, 32, 112, 101, 114, 102, 111, 114, 109, 97, 110, 99, 101, 44, 32, 114, 101, 100, 117, 99, 101, 115, 32, 99, 111, 109, 112, 108, 101, 120, 105, 116, 121, 32, 111, 102, 32, 115, 111, 102, 116, 119, 97, 114, 256, 97, 110, 100, 32, 104, 101, 108, 112, 115, 32, 112, 114, 101, 118, 101, 110, 116, 32, 109, 97, 110, 121, 32, 85, 110, 105, 99, 111, 100, 101, 45, 114, 101, 108, 97, 116, 101, 100, 32, 98, 117, 103, 115, 46, 32, 87, 256, 115, 117, 103, 103, 101, 115, 116, 32, 116, 104, 97, 116, 32, 111, 116, 104, 101, 114, 32, 101, 110, 99, 111, 100, 105, 110, 103, 115, 32, 111, 102, 32, 85, 110, 105, 99, 111, 100, 256, 40, 111, 114, 32, 116, 101, 120, 116, 44, 32, 105, 110, 32, 103, 101, 110, 101, 114, 97, 108, 41, 32, 98, 101, 108, 111, 110, 103, 32, 116, 111, 32, 114, 97, 114, 256, 101, 100, 103, 101, 45, 99, 97, 115, 101, 115, 32, 111, 102, 32, 111, 112, 116, 105, 109, 105, 122, 97, 116, 105, 111, 110, 32, 97, 110, 100, 32, 115, 104, 111, 117, 108, 100, 32, 98, 256, 97, 118, 111, 105, 100, 101, 100, 32, 98, 121, 32, 109, 97, 105, 110, 115, 116, 114, 101, 97, 109, 32, 117, 115, 101, 114, 115, 46]\n",
      "length: 475\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if we are not at the very last position AND the pair matches, replace it\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2 \n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "#print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))\n",
    "\n",
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "print(tokens2)\n",
    "print(\"length:\", len(tokens2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139aba82",
   "metadata": {},
   "source": [
    "We did a single pass through where we replaced 14 pairs (the (101,32) pair) and thus reduced our tokens from 489 - 14 to get 475."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3f65612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into new token 256\n",
      "merging (32, 111) into new token 257\n",
      "merging (100, 32) into new token 258\n",
      "merging (105, 110) into new token 259\n",
      "merging (99, 111) into new token 260\n",
      "merging (114, 32) into new token 261\n",
      "merging (97, 110) into new token 262\n",
      "merging (116, 104) into new token 263\n",
      "merging (97, 116) into new token 264\n",
      "merging (115, 32) into new token 265\n",
      "merging (262, 258) into new token 266\n",
      "merging (101, 110) into new token 267\n",
      "merging (260, 100) into new token 268\n",
      "merging (259, 103) into new token 269\n",
      "merging (116, 32) into new token 270\n",
      "merging (116, 111) into new token 271\n",
      "merging (112, 114) into new token 272\n",
      "merging (257, 102) into new token 273\n",
      "merging (258, 98) into new token 274\n",
      "merging (101, 108) into new token 275\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # A pythonic way to iterate consecutive elements in parallel\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if we are not at the very last position AND the pair matches, replace it\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2 \n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "# ---\n",
    "vocab_size = 276 # our desired final vocab size for this example\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens) # makes a copy so we don't destroy the original list\n",
    "\n",
    "merges = {} # (int, int) -> int | child1, child2 merges\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    print(f\"merging {pair} into new token {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc112280",
   "metadata": {},
   "source": [
    "Notice that even our newly minted tokens are, in turn, eligible for merging as we continue to interate through. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09ca9899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 489\n",
      "ids length: 362\n",
      "compression ratio: 1.35X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"ids length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c11dfe6-0817-4352-bc04-eef0053fd6e0",
   "metadata": {},
   "source": [
    "It's important to remember that a tokenizer is a completely seperate & independent module from the LLM itself. It has its own training dataset/corpus which, in turn, can be completely different than the dataset/corpus used to train the model. Later on the LLM only ever sees the BPE tokens and never deals directly with any text. \n",
    "\n",
    "## Decoding\n",
    "\n",
    "Given a sequence of integers in the range [0, vocab_size], what is the text? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "401ab13c-83ef-4eb6-92b0-dbb524fd47bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1] # adding two byte-objects together, concat\n",
    "\n",
    "def decode(ids):\n",
    "    # given ids (list of integers), returns a Python string\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33913abb-9e62-4fdd-9b65-8df354c640a6",
   "metadata": {},
   "source": [
    "We want to be careful with this current decode implementation. It can throw an error when we pass in `128` for decoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "100ee817-eddb-45bb-ae9b-2407718c077d",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     text \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(ids)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(ids):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# given ids (list of integers), returns a Python string\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(vocab[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m ids)\n\u001b[0;32m----> 8\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1] # adding two byte-objects together, concat\n",
    "\n",
    "def decode(ids):\n",
    "    # given ids (list of integers), returns a Python string\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\")\n",
    "    return text\n",
    "\n",
    "print(decode([128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c181adf0-44ea-443d-92f7-2b46bf9bfc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¿½\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1] # adding two byte-objects together, concat\n",
    "\n",
    "def decode(ids):\n",
    "    # given ids (list of integers), returns a Python string\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\", errors='replace') # this is how we address the potential for decode errors\n",
    "    return text\n",
    "\n",
    "print(decode([128]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74e5fc8-f091-4b4e-82e8-e2cdb639f67b",
   "metadata": {},
   "source": [
    "## Encoding \n",
    "\n",
    "The other way around: Given a string, what are tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbceba41-bfd7-4f6c-a790-ea3500fa9f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(101, 32): 256,\n",
       " (32, 111): 257,\n",
       " (100, 32): 258,\n",
       " (105, 110): 259,\n",
       " (99, 111): 260,\n",
       " (114, 32): 261,\n",
       " (97, 110): 262,\n",
       " (116, 104): 263,\n",
       " (97, 116): 264,\n",
       " (115, 32): 265,\n",
       " (262, 258): 266,\n",
       " (101, 110): 267,\n",
       " (260, 100): 268,\n",
       " (259, 103): 269,\n",
       " (116, 32): 270,\n",
       " (116, 111): 271,\n",
       " (112, 114): 272,\n",
       " (257, 102): 273,\n",
       " (258, 98): 274,\n",
       " (101, 108): 275}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d57101d0-9a6e-4d13-b6fc-685b7524da29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(79, 117): 1,\n",
       " (117, 261): 2,\n",
       " (261, 103): 1,\n",
       " (103, 111): 1,\n",
       " (111, 97): 2,\n",
       " (97, 108): 3,\n",
       " (108, 32): 1,\n",
       " (32, 105): 3,\n",
       " (105, 265): 1,\n",
       " (265, 271): 1,\n",
       " (271, 32): 3,\n",
       " (32, 272): 1,\n",
       " (272, 111): 3,\n",
       " (111, 109): 1,\n",
       " (109, 111): 2,\n",
       " (111, 116): 1,\n",
       " (116, 256): 1,\n",
       " (256, 117): 1,\n",
       " (117, 115): 3,\n",
       " (115, 97): 1,\n",
       " (97, 103): 1,\n",
       " (103, 256): 1,\n",
       " (256, 266): 2,\n",
       " (266, 115): 2,\n",
       " (115, 117): 2,\n",
       " (117, 112): 1,\n",
       " (112, 112): 1,\n",
       " (112, 111): 1,\n",
       " (111, 114): 3,\n",
       " (114, 116): 1,\n",
       " (116, 273): 1,\n",
       " (273, 32): 3,\n",
       " (32, 263): 1,\n",
       " (263, 256): 2,\n",
       " (256, 85): 1,\n",
       " (85, 84): 1,\n",
       " (84, 70): 1,\n",
       " (70, 45): 1,\n",
       " (45, 56): 1,\n",
       " (56, 32): 1,\n",
       " (32, 267): 2,\n",
       " (267, 268): 3,\n",
       " (268, 269): 3,\n",
       " (269, 32): 3,\n",
       " (32, 266): 3,\n",
       " (266, 271): 1,\n",
       " (32, 260): 1,\n",
       " (260, 110): 1,\n",
       " (110, 118): 1,\n",
       " (118, 259): 1,\n",
       " (259, 99): 1,\n",
       " (99, 256): 2,\n",
       " (256, 263): 3,\n",
       " (263, 264): 3,\n",
       " (264, 32): 1,\n",
       " (105, 270): 1,\n",
       " (270, 115): 2,\n",
       " (115, 104): 2,\n",
       " (104, 111): 3,\n",
       " (111, 117): 2,\n",
       " (117, 108): 3,\n",
       " (108, 274): 2,\n",
       " (274, 256): 2,\n",
       " (256, 100): 1,\n",
       " (100, 101): 2,\n",
       " (101, 102): 1,\n",
       " (102, 97): 1,\n",
       " (97, 117): 1,\n",
       " (108, 270): 1,\n",
       " (270, 99): 1,\n",
       " (99, 104): 2,\n",
       " (111, 105): 2,\n",
       " (105, 99): 2,\n",
       " (256, 111): 1,\n",
       " (111, 102): 2,\n",
       " (102, 32): 1,\n",
       " (32, 102): 2,\n",
       " (102, 111): 3,\n",
       " (111, 261): 3,\n",
       " (261, 115): 1,\n",
       " (115, 271): 1,\n",
       " (271, 114): 1,\n",
       " (114, 269): 2,\n",
       " (32, 116): 1,\n",
       " (116, 101): 2,\n",
       " (101, 120): 3,\n",
       " (120, 270): 1,\n",
       " (115, 116): 2,\n",
       " (116, 114): 2,\n",
       " (269, 265): 1,\n",
       " (265, 259): 1,\n",
       " (259, 32): 2,\n",
       " (32, 109): 2,\n",
       " (109, 101): 1,\n",
       " (101, 109): 1,\n",
       " (114, 121): 1,\n",
       " (121, 257): 1,\n",
       " (257, 114): 1,\n",
       " (114, 257): 1,\n",
       " (257, 110): 1,\n",
       " (110, 32): 3,\n",
       " (32, 100): 1,\n",
       " (100, 105): 1,\n",
       " (105, 115): 1,\n",
       " (115, 107): 1,\n",
       " (107, 44): 1,\n",
       " (44, 32): 3,\n",
       " (261, 260): 1,\n",
       " (260, 109): 2,\n",
       " (109, 109): 1,\n",
       " (109, 117): 1,\n",
       " (117, 110): 1,\n",
       " (110, 105): 3,\n",
       " (99, 264): 1,\n",
       " (264, 105): 2,\n",
       " (105, 111): 2,\n",
       " (111, 110): 3,\n",
       " (266, 97): 1,\n",
       " (108, 108): 1,\n",
       " (108, 257): 1,\n",
       " (257, 263): 2,\n",
       " (263, 101): 2,\n",
       " (101, 261): 2,\n",
       " (261, 117): 1,\n",
       " (115, 101): 3,\n",
       " (101, 115): 3,\n",
       " (115, 46): 3,\n",
       " (46, 32): 2,\n",
       " (32, 87): 2,\n",
       " (87, 256): 2,\n",
       " (256, 98): 1,\n",
       " (98, 101): 2,\n",
       " (101, 108): 4,\n",
       " (108, 105): 1,\n",
       " (105, 101): 1,\n",
       " (101, 118): 2,\n",
       " (118, 256): 1,\n",
       " (264, 257): 2,\n",
       " (257, 117): 1,\n",
       " (261, 97): 1,\n",
       " (97, 112): 1,\n",
       " (112, 272): 1,\n",
       " (97, 99): 1,\n",
       " (104, 32): 1,\n",
       " (105, 109): 2,\n",
       " (109, 272): 1,\n",
       " (111, 118): 1,\n",
       " (118, 101): 1,\n",
       " (101, 265): 2,\n",
       " (265, 112): 1,\n",
       " (112, 101): 1,\n",
       " (101, 114): 3,\n",
       " (114, 102): 1,\n",
       " (114, 109): 1,\n",
       " (109, 262): 2,\n",
       " (262, 99): 1,\n",
       " (99, 101): 2,\n",
       " (101, 44): 1,\n",
       " (32, 114): 2,\n",
       " (114, 101): 3,\n",
       " (101, 100): 2,\n",
       " (100, 117): 1,\n",
       " (117, 99): 1,\n",
       " (265, 260): 1,\n",
       " (109, 112): 1,\n",
       " (112, 108): 1,\n",
       " (108, 101): 1,\n",
       " (120, 105): 1,\n",
       " (105, 116): 1,\n",
       " (116, 121): 1,\n",
       " (121, 273): 1,\n",
       " (32, 115): 1,\n",
       " (115, 111): 1,\n",
       " (102, 116): 1,\n",
       " (116, 119): 1,\n",
       " (119, 97): 1,\n",
       " (97, 114): 2,\n",
       " (114, 256): 2,\n",
       " (266, 104): 1,\n",
       " (104, 101): 1,\n",
       " (108, 112): 1,\n",
       " (112, 265): 1,\n",
       " (265, 272): 1,\n",
       " (272, 101): 1,\n",
       " (118, 267): 1,\n",
       " (267, 270): 1,\n",
       " (270, 109): 1,\n",
       " (262, 121): 1,\n",
       " (121, 32): 2,\n",
       " (32, 85): 2,\n",
       " (85, 110): 2,\n",
       " (105, 268): 2,\n",
       " (268, 101): 1,\n",
       " (101, 45): 2,\n",
       " (45, 114): 1,\n",
       " (108, 264): 1,\n",
       " (264, 101): 1,\n",
       " (101, 274): 2,\n",
       " (274, 117): 1,\n",
       " (117, 103): 2,\n",
       " (103, 115): 1,\n",
       " (256, 115): 1,\n",
       " (103, 103): 1,\n",
       " (103, 101): 2,\n",
       " (115, 270): 1,\n",
       " (270, 263): 1,\n",
       " (261, 267): 1,\n",
       " (269, 115): 1,\n",
       " (115, 273): 2,\n",
       " (268, 256): 1,\n",
       " (256, 40): 1,\n",
       " (40, 111): 1,\n",
       " (261, 116): 1,\n",
       " (120, 116): 1,\n",
       " (116, 44): 1,\n",
       " (32, 259): 1,\n",
       " (32, 103): 1,\n",
       " (103, 267): 1,\n",
       " (267, 101): 1,\n",
       " (114, 97): 2,\n",
       " (108, 41): 1,\n",
       " (41, 32): 1,\n",
       " (32, 98): 1,\n",
       " (108, 111): 1,\n",
       " (110, 103): 1,\n",
       " (103, 32): 1,\n",
       " (32, 271): 1,\n",
       " (256, 101): 1,\n",
       " (100, 103): 1,\n",
       " (45, 99): 1,\n",
       " (99, 97): 1,\n",
       " (97, 115): 1,\n",
       " (273, 257): 1,\n",
       " (257, 112): 1,\n",
       " (112, 116): 1,\n",
       " (116, 105): 1,\n",
       " (109, 105): 1,\n",
       " (105, 122): 1,\n",
       " (122, 264): 1,\n",
       " (256, 97): 1,\n",
       " (97, 118): 1,\n",
       " (118, 111): 1,\n",
       " (105, 100): 1,\n",
       " (274, 121): 1,\n",
       " (109, 97): 1,\n",
       " (97, 259): 1,\n",
       " (259, 115): 1,\n",
       " (101, 97): 1,\n",
       " (97, 109): 1,\n",
       " (109, 32): 1,\n",
       " (32, 117): 1,\n",
       " (114, 115): 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "971de72a-0cbb-4a63-a003-b0bdf7f1ec2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 275, 108, 111, 32, 119, 111, 114, 108, 100, 33]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    # given a string, return list of integers (the tokens)\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while True:\n",
    "        stats = get_stats(tokens)\n",
    "        \"\"\"\n",
    "        Here we're iterating using python's min function over an iterator. Remembering that `stats` is a dictionary our example, \n",
    "        python's min with an iterator allows us to iterate over the keys of the stats dictionary. The key lambda takes the indexes\n",
    "        from `merges` as our key for min\n",
    "        \"\"\"\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\"))) \n",
    "        if pair not in merges:\n",
    "            break # nothing else can be merged\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "        \n",
    "\n",
    "print(encode(\"hello world!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea606c2e-9cf9-4c90-b1ed-8f5c1831b7e1",
   "metadata": {},
   "source": [
    "This is good and it works, but it turns out we are leaving out a special case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d0ae59f-e96b-43df-8217-a3406ab70df5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "min() iterable argument is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m merge(tokens, pair, idx)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m, in \u001b[0;36mencode\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      5\u001b[0m stats \u001b[38;5;241m=\u001b[39m get_stats(tokens)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03mHere we're iterating using python's min function over an iterator. Remembering that `stats` is a dictionary our example, \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mpython's min with an iterator allows us to iterate over the keys of the stats dictionary. The key lambda takes the indexes\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mfrom `merges` as our key for min\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerges\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pair \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m merges:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m \u001b[38;5;66;03m# nothing else can be merged\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: min() iterable argument is empty"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    # given a string, return list of integers (the tokens)\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while True:\n",
    "        stats = get_stats(tokens)\n",
    "        \"\"\"\n",
    "        Here we're iterating using python's min function over an iterator. Remembering that `stats` is a dictionary our example, \n",
    "        python's min with an iterator allows us to iterate over the keys of the stats dictionary. The key lambda takes the indexes\n",
    "        from `merges` as our key for min\n",
    "        \"\"\"\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\"))) \n",
    "        if pair not in merges:\n",
    "            break # nothing else can be merged\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "        \n",
    "\n",
    "print(encode(\"h\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eb85de-f19c-4cff-b351-56cbd1799542",
   "metadata": {},
   "source": [
    "If we only have a single character or a completely empty string, then `stats` becomes empty and causes an error inside `min`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08dc4fce-e411-4699-8201-87b1ea940468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 275, 108, 111, 32, 119, 111, 114, 108, 100, 33]\n",
      "[104]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    # given a string, return list of integers (the tokens)\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while len(tokens) >= 2:\n",
    "        stats = get_stats(tokens)\n",
    "        \"\"\"\n",
    "        Here we're iterating using python's min function over an iterator. Remembering that `stats` is a dictionary our example, \n",
    "        python's min with an iterator allows us to iterate over the keys of the stats dictionary. The key lambda takes the indexes\n",
    "        from `merges` as our key for min\n",
    "        \"\"\"\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\"))) \n",
    "        if pair not in merges:\n",
    "            break # nothing else can be merged\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "        \n",
    "\n",
    "print(encode(\"hello world!\"))\n",
    "print(encode(\"h\"))\n",
    "print(encode(\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1ee173-579e-4d3e-9db0-acb0f84fb087",
   "metadata": {},
   "source": [
    "If we encoded a string and then immediately decode it back you expect to get the same string returned. But is that true for all strings? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af523034-573e-4e68-b3f7-8c13a986da44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"hello world\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c756b6-549d-4d7e-8941-7714a975a8b5",
   "metadata": {},
   "source": [
    "While this may typically work, it's important to still remember that not all token sequences are valid UTF-8 byte streams. Thus, in those cases they wouldn't be decodable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41565488-9be9-4449-9fdf-dd1b7fe76a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "text2 = decode(encode(text))\n",
    "print(text2 == text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad044d1-4756-4ac6-9580-6ab7068f5ac7",
   "metadata": {},
   "source": [
    "Let's go ahead and start looking at some more SOTA LLM's and the kinds of tokenizers that they use. \n",
    "\n",
    "## Forced splits using regex patterns (GPT-2)\n",
    "Start by reading \"Language Models are Unsupervised Multitask Learners\" (Radford et al. 2018)(aka, the GPT-2 paper), specifically the section Input Representation. In it, they mention that they encountered issues with the naive byte-level implementation of BPE that we've worked with so far in this notebook. They observed the implementation including many versions of common words like dog, since they occur in many variations such as \"dog.\", \"dog!\", \"dog?\", etc. This resulted in sub-optimal allocation of limited vocabulary slots and model capacity. \n",
    "\n",
    "To avoid this, they prevent BPE from merging across character categories for any given byte sequence. They also state that they added an exception for spaces which significantly improved their compression efficiency while adding minimal fragmentation of words across multiple vocab tokens. \n",
    "\n",
    "Below is the regex pattern they use in gpt-2's encoder.py (the tokenizer), which can be found on github. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c38c02b5-2897-48f2-b69e-2eb1b527e684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(gpt2pat, \"Hello world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4393c07-3017-44c3-a23c-8bd9a8c51fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world', '123', ' how', ' are', ' you']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(gpt2pat, \"Hello world123 how are you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0ddd23d-9688-4a28-9b3e-34c22679c170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world', '123', ' how', \"'ve\", ' you', ' been']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(gpt2pat, \"Hello world123 how've you been\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc9d6810-e3a9-49d8-8d40-dbfdf8868384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world', '123', ' how', \"'ve\", ' you', ' been', '!?!?']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(gpt2pat, \"Hello world123 how've you been!?!?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22ef583a-cd97-4d0c-bf8b-61d0c135fc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world', '123', ' how', \"'ve\", ' you', '    ', ' been', '!?!?', '    ']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(gpt2pat, \"Hello world123 how've you     been!?!?    \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674ebdb-0bad-4605-87c3-289b750a9f7d",
   "metadata": {},
   "source": [
    "The regex appears to take a raw string (r\"\"\") and looks for many 'x suffixes using many regex \"or\" symbols (|). \n",
    "\n",
    "We see then see \" ?\\p{L}+\" which allows an optional space followed by one or more letters of any kind from any language. \n",
    "\n",
    "\" ?\\p{N}+\" means an optional space followed by any kind of numeric character in any script. \n",
    "\n",
    "We then see \" ?[^\\s\\p{L}\\p{N}]+\" where we get another optional space followed by something that is not a letter or number, essentially trying to match punctuation. Lastly we see \"\\s+(?!\\S)|\\s+\" which uses a **negative lookahead assertion** to match white space up to *but not including* the last white space character. \n",
    "\n",
    "Regex patterns like this one are one way you can enforce rules prohibiting certain kinds of merges when it comes to chunking the text.  \n",
    "\n",
    "It's important to notice that OpenAI comment in their code that they \"Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\" because their regex pattern only matches for the lowercase conditions that they wrote. Moreover the fundamental use of apostrophes is potentially english-language specific, but I've not rigourously investigated this enough to determine how severe of a limitation it is in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5211f215-a93b-4055-a7e2-e863ca4a93f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'for', ' i', ' in', ' range', '(', '1', ',', ' 101', '):', '\\n   ', ' if', ' i', ' %', ' 3', ' ==', '0', ' and', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'FizzBuzz', '\")', '\\n   ', ' elif', ' i', ' %', ' 3', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Fizz', '\")', '\\n   ', ' elif', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Buzz', '\")', '\\n   ', ' else', ':', '\\n       ', ' print', '(', 'i', ')', '\\n']\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"\n",
    "for i in range(1, 101):\n",
    "    if i % 3 ==0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\n",
    "\"\"\"\n",
    "print(re.findall(gpt2pat, example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58447474-f8ba-483b-84bd-7af510601df3",
   "metadata": {},
   "source": [
    "In practice, OpenAI did enforce some sort of rule to prevent the merging of these space chunks when they trained their tokenizer for gpt-2. Unfortunately, they are not very clear about how they implemented said rule as the training code has yet to ever be released. One can see this is the case by going to `https://tiktokenizer.vercel.app/`, selecting gpt-2 encodings, and noticing that spaces are individually tokenized. \n",
    "\n",
    "## Tiktoken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a46def54-6831-4e92-bff5-c3c803706644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 23748, 995, 10185]\n",
      "[262, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# GPT-2 (does not merge spaces)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"    hello world!!!\"))\n",
    "\n",
    "# GPT-4 (merges spaces)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"    hello world!!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae14401-e958-453c-8b4d-968340ae0594",
   "metadata": {},
   "source": [
    "Tiktoken is OpenAI's official tokenizer, and `cl100k_base` is the one used for GPT-4. Unfortunately again, OpenAI has only published tiktoken's inference code and not its training code. They also changed the regex pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90921f48-992e-4240-849e-7b30832d3879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world', '123', ' how', \"'ve\", ' you', '    ', ' been', '!?!?', '    ']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "gpt4pat = re.compile(r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(gpt4pat, \"Hello world123 how've you     been!?!?    \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358b49c-6bb3-418b-aba0-6df32f11a30b",
   "metadata": {},
   "source": [
    "With this regex pattern, we can see some pretty stark differences. We still match a Python raw string literal with r\"\"\"\n",
    "\n",
    "We then see '(?i:[sdmt]|ll|ve|re) where ' matches a single quote, (?i:...) indicates case-insensitive matching for the contents, [sdmt]|ll|ve|re matches 's', 'd', 'm', 't', 'll','ve', or 're' in order to handle contractions\n",
    "\n",
    "| indicates an \"or\" like we saw before\n",
    "\n",
    "With [^\\r\\n\\p{L}\\p{N}]?+, we see that [^...] negates the character sets \\r (return) \\n (new line), \\p{L} (any kind of letter), \\p{N} any kind of numeric character, and ?+ for a possive optional quantifier of any single character that is not a letter, number, or line break. \n",
    "\n",
    "\\p{L}+ for one or more letters from any language like we saw with GPT-2's regex pattern\n",
    "\n",
    "\\p{N}{1,3} matches 1 to 3 numeric characters\n",
    "\n",
    "?[^\\s\\p{L}\\p{N}]++ indicating any optional space followed by a match of any character that is not whitespace, a letter, or number. ++ for possive one or more quantifier matching any non-alphanumeric, non-whitespace character(s), which could possibly be preceded by a space. \n",
    "\n",
    "[\\r\\n]* matches zero or more carriage returns or new lines. \n",
    "\n",
    "\\s*[\\r\\n] matches any whitespace followed by a line break.\n",
    "\n",
    "\\s+(?!\\S), where \\s+ matches one or more whitespace characters and (?!\\S) uses negative lookahead for any non-whitespace character and essentially matches whitespace at the end of a string. \n",
    "\n",
    "Finally \\s+ again matches one ore more whitespace characters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c12ff23-1ee4-4eea-b30e-0f8e5e91bf4b",
   "metadata": {},
   "source": [
    "## Special Tokens\n",
    "\n",
    "Special tokens help us delimit certain parts of the data or create special parts of a structure of the token strings.\n",
    "\n",
    "One example of a commonly used special token is the EOT (or End Of Text) token, which is inserted at the end of strings or documents during training to indicate to the LLM that the document has ended and a new document is starting. This and other special tokens are essential for fine-tuning a foundation LLM model into something like a chatbot, etc. \n",
    "\n",
    "## Sentencepiece\n",
    "\n",
    "Sentencepiece is commonly used in other LLM's like the Llama series, T5, Mistral series and others. The reason being is that, unlike tiktoken, it can efficiently both train and inference BPE tokenizers AND it runs BPE on Unicode code points directly! \n",
    "\n",
    "Tiktoken encodes to utf-8 and then BPE's bytes, whereas sentencepiece BPE's the code points and optionally falls back to utf-8 for rare code points (where rarity is determined by the character_coverage hyperparameter), which then get translated to byte tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072eb3f7-58b2-4083-a37f-b1dac2f5c7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
